{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.tokenize.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPFa57sVCnxndmIQD9Erzd9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinwoo3239/nlp_study_basic/blob/main/1_tokenize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference:\n",
        "\n",
        "[[Wikidoc_NLP using deeplearning]](https://wikidocs.net/book/2155)"
      ],
      "metadata": {
        "id": "TubdRDr7o4Ih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "\n",
        "- Tokenization: spliting corpus into token (meaningful unit)\n",
        "- cleaning: Removing data noises from corpus\n",
        "- normalization: Words with different expressions are combined to form the same word."
      ],
      "metadata": {
        "id": "XqZMtJeupjCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer, TreebankWordTokenizer\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# nltk.download('popular')"
      ],
      "metadata": {
        "id": "vzQuElV8pm4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\""
      ],
      "metadata": {
        "id": "KuuW2f38psX_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(text))\n",
        "print('-'*20)\n",
        "print(text_to_word_sequence(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IeUTrJerzJn",
        "outputId": "e6d01957-27ba-4597-9243-52c8ccb2412c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal', '.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n",
            "--------------------\n",
            "['starting', 'a', 'home', 'based', 'restaurant', 'may', 'be', 'an', 'ideal', 'it', \"doesn't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = WordPunctTokenizer()\n",
        "print(tokenizer.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0W5JUysr0pi",
        "outputId": "4bc53be1-cafc-41a1-fa84-d23b68971b0d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Starting', 'a', 'home', '-', 'based', 'restaurant', 'may', 'be', 'an', 'ideal', '.', 'it', 'doesn', \"'\", 't', 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 대, 소문자 통합 - 무작정 제거하면 안됨\n",
        "3. 불필요한 단어의 제거 - 빈도수가 낮거나, 너무 짧은 단어"
      ],
      "metadata": {
        "id": "f_4BtN4tsxHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopwords"
      ],
      "metadata": {
        "id": "73qkRCm6s9Mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "# from konlpy.tag import Okt"
      ],
      "metadata": {
        "id": "OY-QB3KAs_Gs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_list = stopwords.words('english')\n",
        "print('불용어 개수 :', len(stop_words_list))\n",
        "print('불용어 10개 출력 :',stop_words_list[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8t3OqlavnEA",
        "outputId": "c3ad2ba0-a5e1-4c05-b47a-0fad59afa721"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 개수 : 179\n",
            "불용어 10개 출력 : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"Family is not an important thing. It's everything.\"\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "word_tokens = word_tokenize(example)\n",
        "\n",
        "result = []\n",
        "for word in word_tokens: \n",
        "    if word not in stop_words: \n",
        "        result.append(word) \n",
        "\n",
        "print('불용어 제거 전 :',word_tokens) \n",
        "print('불용어 제거 후 :',result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgUJgLO4vpgZ",
        "outputId": "09a8fc61-06da-4f85-c767-eae93d6717ae"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
            "불용어 제거 후 : ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Count based word Representation\n",
        " DTM(Document Term Matrix)과 TF-IDF(Term Frequency-Inverse Document Frequency)"
      ],
      "metadata": {
        "id": "EaISFuYdv6-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of Words란 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법입니다. Bag of Words를 직역하면 단어들의 가방이라는 의미입니다. 단어들이 들어있는 가방을 상상해봅시다. 갖고있는 어떤 텍스트 문서에 있는 단어들을 가방에다가 전부 넣습니다. 그 후에는 이 가방을 흔들어 단어들을 섞습니다. 만약, 해당 문서 내에서 특정 단어가 N번 등장했다면, 이 가방에는 그 특정 단어가 N개 있게됩니다. 또한 가방을 흔들어서 단어를 섞었기 때문에 더 이상 단어의 순서는 중요하지 않습니다.\n",
        "\n",
        "문서 단어 행렬(Document-Term Matrix, DTM)이란 다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현한 것을 말합니다. 쉽게 생각하면 각 문서에 대한 BoW를 하나의 행렬로 만든 것으로 생각할 수 있으며, BoW와 다른 표현 방법이 아니라 BoW 표현을 다수의 문서에 대해서 행렬로 표현하고 부르는 용어입니다. 예를 들어서 이렇게 4개의 문서가 있다고 합시다.\n",
        "(문서, "
      ],
      "metadata": {
        "id": "vNG_2N1Pz52A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords "
      ],
      "metadata": {
        "id": "8ElBKbTK0GWr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"Family is not an important thing. It's everything.\"]"
      ],
      "metadata": {
        "id": "5tak4Zr33OAW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vect = CountVectorizer()\n",
        "vect.fit_transform(text)\n",
        "\n",
        "sequence = vect.transform(text).toarray()\n",
        "print(\"bag of words vector :\", sequence)\n",
        "print(\"Vocabulary:\", vect.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmNazJik3OGt",
        "outputId": "1b034f78-a205-4feb-92e4-7cf1d5c7b3dc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bag of words vector : [[1 1 1 1 1 1 1 1]]\n",
            "Vocabulary: {'family': 2, 'is': 4, 'not': 6, 'an': 0, 'important': 3, 'thing': 7, 'it': 5, 'everything': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vect = CountVectorizer(stop_words=\"english\")\n",
        "# vect = CountVectorizer(stop_words=[\"the\", \"a\", \"an\", \"is\", \"not\"])\n",
        "\n",
        "print('bag of words vector :',vect.fit_transform(text).toarray())\n",
        "print('vocabulary :',vect.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xgkw8cNu3OLk",
        "outputId": "3d3cdfa7-2b8b-4f5c-9e1a-5e77d10b836e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bag of words vector : [[1 1 1]]\n",
            "vocabulary : {'family': 0, 'important': 1, 'thing': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words(\"english\")\n",
        "vect = CountVectorizer(stop_words=stop_words)\n",
        "\n",
        "print('bag of words vector :',vect.fit_transform(text).toarray()) \n",
        "print('vocabulary :',vect.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncasjjaS3Gp_",
        "outputId": "f57192f5-ccb0-4db3-fcac-c911b2579219"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bag of words vector : [[1 1 1 1]]\n",
            "vocabulary : {'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF"
      ],
      "metadata": {
        "id": "2uilyPvo4CW_"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    'you know I want your love',\n",
        "    'I like you',\n",
        "    'what should I do ',    \n",
        "]\n",
        "\n",
        "vector = CountVectorizer()\n",
        "\n",
        "# 코퍼스로부터 각 단어의 빈도수를 기록\n",
        "print(vector.fit_transform(corpus).toarray())\n",
        "\n",
        "# 각 단어와 맵핑된 인덱스 출력\n",
        "print(vector.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOCwcHZd4Zuh",
        "outputId": "80b884b2-950d-4aaf-d119-8ea4797ab07b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 0 1 0 1 0 1 1]\n",
            " [0 0 1 0 0 0 0 1 0]\n",
            " [1 0 0 0 1 0 1 0 0]]\n",
            "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = [\n",
        "    'you know I want your love',\n",
        "    'I like you',\n",
        "    'what should I do ',    \n",
        "]\n",
        "\n",
        "tfidfv = TfidfVectorizer().fit(corpus)\n",
        "print(tfidfv.transform(corpus).toarray())\n",
        "print(tfidfv.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2KtjW8s4cfT",
        "outputId": "0e596cd2-945e-4201-f506-d68b9066a53e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
            "  0.         0.35543247 0.46735098]\n",
            " [0.         0.         0.79596054 0.         0.         0.\n",
            "  0.         0.60534851 0.        ]\n",
            " [0.57735027 0.         0.         0.         0.57735027 0.\n",
            "  0.57735027 0.         0.        ]]\n",
            "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n"
      ],
      "metadata": {
        "id": "cR06ZXP44h2f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}